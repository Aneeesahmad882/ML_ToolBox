{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "799072c9-3dac-4441-93e1-28f89d9f758f",
   "metadata": {},
   "source": [
    "# Answer1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "543cc9a2-ae2b-448e-9281-b84f2d74e4a4",
   "metadata": {},
   "source": [
    "Analysis of Variance (ANOVA) is a statistical method used to compare means among three or more groups. To ensure the validity of ANOVA results, certain assumptions must be met. Violations of these assumptions can impact the accuracy and reliability of the analysis. The key assumptions for ANOVA are:\n",
    "\n",
    "1. Independence of Observations:\n",
    "   - Assumption: The observations in each group must be independent of each other.\n",
    "   - Violation: If observations within groups are not independent (e.g., repeated measures on the same subjects or if there is clustering), it can lead to biased results.\n",
    "\n",
    "2.Normality:\n",
    "   - Assumption: The residuals (the differences between observed and predicted values) should be normally distributed.\n",
    "   - Violation: If the residuals are not normally distributed, it can affect the accuracy of the p-values and confidence intervals      associated with the F-test.\n",
    "\n",
    "3. Homogeneity of Variances (Homoscedasticity):\n",
    "   - Assumption: The variances of the residuals should be approximately equal across all groups.\n",
    "   - Violation: Heteroscedasticity, where the variances are not equal, can lead to imprecise results. This is especially important because ANOVA is sensitive to unequal variances, and it can affect the Type I error rate.\n",
    "\n",
    "4. Interval or Ratio Scale of Measurement:\n",
    "   - Assumption: The dependent variable should be measured on an interval or ratio scale.\n",
    "   - Violation: If the dependent variable is measured on a nominal or ordinal scale, using ANOVA would be inappropriate. It might be more suitable to use non-parametric tests in such cases.\n",
    "\n",
    "Examples of violations:\n",
    "\n",
    "- Outliers:\n",
    "  - Violation: Presence of extreme values in one or more groups.\n",
    "  - Impact: Outliers can distort the means and affect the homogeneity of variances assumption.\n",
    "\n",
    "- Non-Normality:\n",
    "  - Violation: Residuals do not follow a normal distribution.\n",
    "  - Impact: Skewed or heavy-tailed distributions can lead to inaccurate p-values and confidence intervals.\n",
    "\n",
    "- Heteroscedasticity:\n",
    "  - Violation: Variances of residuals are not equal across groups.\n",
    "  - Impact: It can lead to incorrect standard errors and affect the F-test's validity.\n",
    "\n",
    "- Non-Independence:\n",
    "  - Violation: Observations within groups are not independent.\n",
    "  - Impact: Correlated observations may lead to underestimation of variability and inflated significance.\n",
    "\n",
    "It's important to note that ANOVA is robust to violations of assumptions, especially when sample sizes are large. However, if violations are severe, alternative methods or transformations of the data may be considered. Additionally, exploratory data analysis and diagnostic checks (such as residual plots) can help assess the validity of ANOVA results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d3d90-5073-406a-b10a-ee84ad1f8aab",
   "metadata": {},
   "source": [
    "# Answer2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46286667-dba6-491f-a011-86df622b815b",
   "metadata": {},
   "source": [
    "Analysis of Variance (ANOVA) comes in several forms, and the choice of which to use depends on the specific experimental design and the nature of the data. The three main types of ANOVA are:\n",
    "\n",
    "1. One-Way ANOVA:\n",
    "   - Use Case: Used when there is one independent variable (factor) with more than two levels or groups.\n",
    "   - Example: Suppose you have three different teaching methods, and you want to compare their effectiveness in improving test scores. The teaching method would be the independent variable with three levels (Group 1, Group 2, Group 3), and the test scores would be the dependent variable.\n",
    "\n",
    "2. Two-Way ANOVA:\n",
    "   - Use Case: Used when there are two independent variables, and you want to examine their individual and interactive effects on the dependent variable.\n",
    "   - Example: Consider a study where you are investigating the effects of both gender and a new drug on blood pressure. Gender (male/female) and the drug (present/absent) would be the two independent variables.\n",
    "\n",
    "3. Repeated Measures ANOVA:\n",
    "   - Use Case: Used when the same subjects are used for each treatment or measurement condition, and the measurements are taken at multiple time points or under different conditions.\n",
    "   - Example: If you are conducting a study where you measure blood pressure in the same individuals before and after the administration of a drug, the repeated measurements on the same subjects would be analyzed using repeated measures ANOVA.\n",
    "\n",
    "Each type of ANOVA has its own assumptions and requirements, and the choice between them depends on the experimental design and the research questions of interest. Additionally, there are variations and extensions of ANOVA, such as mixed-design ANOVA, which combines features of both one-way and repeated measures ANOVA, allowing for the examination of both between-subjects and within-subjects effects. Researchers need to carefully consider the design of their study and the nature of their data to determine the most appropriate type of ANOVA to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2216d54-8372-4023-99a8-fa36c96a956a",
   "metadata": {},
   "source": [
    "# Answer3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68fbd5ae-1ace-49af-989f-0a2908cda9a3",
   "metadata": {},
   "source": [
    "The partitioning of variance in Analysis of Variance (ANOVA) refers to the division of the total variance observed in the data into different components associated with various sources or factors. Understanding this concept is crucial because it provides insight into the sources of variability in the data and helps assess the significance of these sources. The total variance in the data is decomposed into several components, typically including:\n",
    "\n",
    "1. Total Variance (Total Sum of Squares, SST):\n",
    "   - This represents the overall variability in the dependent variable across all groups. It is calculated as the sum of the squared differences between each observed data point and the overall mean.\n",
    "\n",
    "2. Between-Group Variance (Between-Group Sum of Squares, SSB):\n",
    "   - This represents the variability in the dependent variable that is attributed to differences between the group means. It is calculated as the sum of the squared differences between each group mean and the overall mean, weighted by the number of observations in each group.\n",
    "\n",
    "3. Within-Group Variance (Within-Group Sum of Squares, SSW or SSE):\n",
    "   - This represents the variability in the dependent variable that is due to individual differences within each group. It is calculated as the sum of the squared differences between each individual data point and its group mean.\n",
    "\n",
    "The key idea is that if the between-group variance is significantly larger than the within-group variance, it suggests that there are systematic differences between the groups. This forms the basis for the F-test in ANOVA, where the ratio of between-group variance to within-group variance is examined.\n",
    "\n",
    "The F-statistic is calculated as:\n",
    "\n",
    "F = (Between-Group Variance)/(Within-Group Variance)\n",
    "\n",
    "If the F-statistic is sufficiently large, it implies that the observed differences between group means are unlikely to have occurred by chance, and the null hypothesis of equal group means is rejected.\n",
    "\n",
    "Understanding the partitioning of variance is important for several reasons:\n",
    "\n",
    "1. Interpretation of Group Differences: It allows researchers to quantify the proportion of variance in the dependent variable that can be attributed to differences between groups.\n",
    "\n",
    "2. Hypothesis Testing: It forms the basis for hypothesis testing in ANOVA. By comparing the between-group variance to the within-group variance, researchers can determine whether the group means are significantly different.\n",
    "\n",
    "3. Effect Size: The partitioning of variance provides a basis for calculating effect size measures, such as eta-squared ( eta^2 ), which indicate the proportion of total variance in the dependent variable explained by group membership.\n",
    "\n",
    "Overall, understanding the partitioning of variance in ANOVA enhances the interpretability and reliability of the results obtained from the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d30ec-9741-45a8-b32e-678e1566f8c9",
   "metadata": {},
   "source": [
    "# Answer4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bfe82c0-a33d-4958-ae00-117719eac18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 269.6\n",
      "Explained Sum of Squares (SSE): 194.79999999999998\n",
      "Residual Sum of Squares (SSR): 74.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Example data\n",
    "group1 = np.array([12, 14, 18, 16, 20])\n",
    "group2 = np.array([10, 8, 12, 11, 14])\n",
    "group3 = np.array([5, 7, 6, 8, 10])\n",
    "\n",
    "# Combine data into a single array\n",
    "all_data = np.concatenate([group1, group2, group3])\n",
    "\n",
    "# Calculate overall mean\n",
    "overall_mean = np.mean(all_data)\n",
    "\n",
    "# Calculate Total Sum of Squares (SST)\n",
    "sst = np.sum((all_data - overall_mean)**2)\n",
    "\n",
    "# Calculate group means\n",
    "mean_group1 = np.mean(group1)\n",
    "mean_group2 = np.mean(group2)\n",
    "mean_group3 = np.mean(group3)\n",
    "\n",
    "# Calculate Explained Sum of Squares (SSE)\n",
    "sse = len(group1) * (mean_group1 - overall_mean)**2 + \\\n",
    "      len(group2) * (mean_group2 - overall_mean)**2 + \\\n",
    "      len(group3) * (mean_group3 - overall_mean)**2\n",
    "\n",
    "# Calculate Residual Sum of Squares (SSR)\n",
    "ssr = np.sum((group1 - mean_group1)**2) + \\\n",
    "      np.sum((group2 - mean_group2)**2) + \\\n",
    "      np.sum((group3 - mean_group3)**2)\n",
    "\n",
    "# Verify that SST equals SSE + SSR\n",
    "assert np.isclose(sst, sse + ssr)\n",
    "\n",
    "# Output results\n",
    "print(\"Total Sum of Squares (SST):\", sst)\n",
    "print(\"Explained Sum of Squares (SSE):\", sse)\n",
    "print(\"Residual Sum of Squares (SSR):\", ssr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d205e8-3907-4916-9a4a-af71b6486785",
   "metadata": {},
   "source": [
    "# Answer5"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74d3ed28-a8d5-4012-afc2-acde6bf6851b",
   "metadata": {},
   "source": [
    "In a two-way ANOVA, there are main effects associated with each independent variable (factor) and an interaction effect that represents the combined influence of the two factors. The main effects reflect the average effect of each factor across all levels of the other factor, while the interaction effect indicates whether the effect of one factor depends on the level of the other factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b46556-294a-445f-acd3-c88c9757a7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effect A: 413.20054676820814\n",
      "Main Effect B: 35.52960659726933\n",
      "Interaction Effect: 62.852522826188086\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'A': [10, 15, 20, 25, 30, 12, 18, 24, 30, 36],\n",
    "    'B': [5, 10, 15, 20, 25, 20, 15, 10, 5, 0],\n",
    "    'Y': [25, 30, 35, 40, 45, 20, 25, 30, 35, 40]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "model = ols('Y ~ A * B', data=df).fit()\n",
    "\n",
    "# Perform the ANOVA\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Extract main effects and interaction effect\n",
    "main_effect_A = anova_table['sum_sq']['A'] / anova_table['df']['A']\n",
    "main_effect_B = anova_table['sum_sq']['B'] / anova_table['df']['B']\n",
    "interaction_effect = anova_table['sum_sq']['A:B'] / anova_table['df']['A:B']\n",
    "\n",
    "# Print results\n",
    "print(\"Main Effect A:\", main_effect_A)\n",
    "print(\"Main Effect B:\", main_effect_B)\n",
    "print(\"Interaction Effect:\", interaction_effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c62ed-7390-4a87-8229-1793661390ff",
   "metadata": {},
   "source": [
    "# Answer6"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89f6cf24-6387-42e3-8d84-b4b60d85a281",
   "metadata": {},
   "source": [
    "In a one-way ANOVA, the F-statistic is used to test whether there are significant differences among the means of the groups. The associated p-value indicates the probability of obtaining such results by random chance.\n",
    "\n",
    "In your case, you obtained an F-statistic of 5.23 and a p-value of 0.02. To interpret these results:\n",
    "\n",
    "1. F-Statistic:\n",
    "   - The F-statistic is a ratio of the variability between group means to the variability within groups. In your case, a larger F-statistic suggests that there is more variability between the group means relative to the variability within each group.\n",
    "\n",
    "2. P-Value:\n",
    "   - The p-value of 0.02 is less than the conventional significance level of 0.05. This indicates that the observed differences between group means are statistically significant at the 0.05 significance level.\n",
    "\n",
    "3. Conclusion:\n",
    "   - Based on the obtained results, you would reject the null hypothesis. The null hypothesis in this context typically states that there are no significant differences among the group means. Since the p-value is less than 0.05, you have evidence to suggest that the differences are unlikely to be due to random chance alone.\n",
    "\n",
    "4. Practical Significance:\n",
    "   - While statistical significance is an important consideration, it's also essential to assess the practical significance of the findings. Even if differences are statistically significant, they may not necessarily be practically significant or meaningful in real-world terms. Consider the context of your study and the size of the effect.\n",
    "\n",
    "In summary, with an F-statistic of 5.23 and a p-value of 0.02, you would conclude that there are statistically significant differences among the group means. It suggests that at least one group mean is different from the others. The specific groups that differ would need further investigation, possibly through post hoc tests or additional analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7993e9de-0eb9-4bf7-8b04-61ee5fbf3dab",
   "metadata": {},
   "source": [
    "# Answer7"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7966a132-58cf-4bec-8e1e-3775fad612e5",
   "metadata": {},
   "source": [
    "Handling missing data in a repeated measures ANOVA is crucial to ensure the validity and reliability of the analysis. Different methods exist for dealing with missing data, and the choice of method can have consequences on the results. Here are some common approaches to handle missing data in repeated measures ANOVA and their potential consequences:\n",
    "\n",
    "1. Complete Case Analysis (Listwise Deletion):\n",
    "   - Approach: Exclude cases with missing data for any variable in the analysis.\n",
    "   - Consequences:\n",
    "      - Pros: Simple to implement.\n",
    "      - Cons: Reduces sample size, potentially leading to biased results if missing data are not missing completely at random (MCAR). It may also introduce selection bias.\n",
    "\n",
    "2. Pairwise Deletion:\n",
    "   - Approach: Analyze all available data for each pair of variables, ignoring cases with missing data for some variables.\n",
    "   - Consequences:\n",
    "      - Pros: Retains more data than listwise deletion.\n",
    "      - Cons: Results in different sample sizes for different variable pairs, and can lead to biased results if the missing data pattern is not MCAR. It can also inflate Type I error rates.\n",
    "\n",
    "3. Imputation Methods (e.g., Mean Imputation, Linear Interpolation, etc.):**\n",
    "   - Approach: Replace missing values with estimated values based on observed data.\n",
    "   - Consequences:\n",
    "      - Pros: Retains all cases and may reduce bias compared to deletion methods.\n",
    "      - Cons: Imputed values may not accurately represent the true values, and the assumption that the missing data mechanism is ignorable (e.g., missing at random) is critical. The imputation method itself may introduce bias.\n",
    "\n",
    "4. Mixed-Effects Models (e.g., Linear Mixed-Effects Models):\n",
    "   - Approach: Utilize all available data, estimating fixed effects and random effects simultaneously.\n",
    "   - Consequences:\n",
    "      - Pros: Provides unbiased estimates under the assumption that missing data are missing at random. Retains all available data.\n",
    "      - Cons: Assumes a specific model for the missing data mechanism. Can be computationally intensive.\n",
    "\n",
    "5. Multiple Imputation:\n",
    "   - Approach: Create multiple imputed datasets, analyze each, and combine results.\n",
    "   - Consequences:\n",
    "      - Pros: Accounts for uncertainty in imputation. Can provide more accurate estimates than single imputation methods.\n",
    "      - Cons: Requires careful consideration of the imputation model. The process can be computationally demanding.\n",
    "\n",
    "Choosing the appropriate method depends on the nature of the missing data, the assumptions made about the missing data mechanism, and the goals of the analysis. It's essential to report the method used and consider the potential impact of missing data on the interpretation of results. Sensitivity analyses, where different methods are compared, can help assess the robustness of findings to missing data handling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f667b0-3910-48f8-97b0-da098c478b48",
   "metadata": {},
   "source": [
    "# Answer8"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c418da1b-811b-4be4-b44a-d31df8f61477",
   "metadata": {},
   "source": [
    "After conducting an Analysis of Variance (ANOVA) and finding a significant difference among group means, post-hoc tests are often used to explore specific pairwise comparisons between groups. Some common post-hoc tests include:\n",
    "\n",
    "1. Tukey's Honestly Significant Difference (HSD):\n",
    "   - Use Case: Tukey's HSD is used when you have three or more groups, and it controls the familywise error rate. It is appropriate when you have a balanced design (equal sample sizes in each group).\n",
    "   - Example: Suppose you have conducted a one-way ANOVA to compare the mean scores of students taught using three different teaching methods. Tukey's HSD can be used to identify which specific pairs of teaching methods have significantly different means.\n",
    "\n",
    "2. Bonferroni Correction:\n",
    "   - Use Case: Bonferroni correction is a conservative approach that adjusts the significance level to control the familywise error rate. It is suitable for situations where multiple pairwise comparisons are made.\n",
    "   - Example: In a study comparing the effectiveness of four different medications, a one-way ANOVA might reveal a significant overall difference. Bonferroni correction can then be applied to examine specific pairs of medications to identify where the differences lie.\n",
    "\n",
    "3. Scheffé's Method:\n",
    "   - Use Case: Scheffé's method is a robust post-hoc test that can be used in cases where sample sizes are unequal or groups have different variances. It provides a wider confidence interval compared to Tukey's HSD.\n",
    "   - Example: Imagine a study comparing the performance of students from three different schools on a standardized test. If the ANOVA indicates a significant overall difference, Scheffé's method can help identify which pairs of schools have significantly different mean scores.\n",
    "\n",
    "4. Dunnett's Test:\n",
    "   - Use Case: Dunnett's test is appropriate when you have a control group, and you want to compare other groups to the control. It controls the overall Type I error rate.\n",
    "   - Example: In a medical trial, a one-way ANOVA might be used to compare the effectiveness of a new drug across multiple dosages with a placebo as the control group. Dunnett's test can be applied to compare each dosage group with the placebo group.\n",
    "\n",
    "5. Games-Howell Test:\n",
    "   - Use Case: The Games-Howell test is a post-hoc test suitable for situations with unequal variances and/or unequal sample sizes. It does not assume equal variances across groups.\n",
    "   - Example: In a study comparing the performance of different age groups on a cognitive task, if the ANOVA reveals a significant difference, the Games-Howell test can be used to make pairwise comparisons between age groups.\n",
    "\n",
    "6. Holm's Method:\n",
    "   - Use Case: Holm's method is a stepwise procedure that controls the familywise error rate and is less conservative than Bonferroni. It is appropriate when conducting multiple pairwise tests.\n",
    "   - Example: In a study with multiple treatment groups, Holm's method can be applied after ANOVA to identify specific treatment pairs that differ in terms of their mean effects.\n",
    "\n",
    "The choice of post-hoc test depends on the characteristics of the data, including the number of groups, sample sizes, homogeneity of variances, and the specific research question. It's important to carefully select a post-hoc test that is suitable for your study design and to report the chosen method along with the ANOVA results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b710d6f0-0db5-4b5b-87b3-d56b57f82f47",
   "metadata": {},
   "source": [
    "# Answer9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af78b184-9cfe-40e4-802b-f8972c19b704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Way ANOVA Results:\n",
      "F-Statistic: 21.809565795751933\n",
      "P-Value: 5.076768176045347e-09\n",
      "\n",
      "There is a significant difference in mean weight loss between at least two diets.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Example data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "diet_A = np.random.normal(loc=2, scale=1, size=50)  # Mean weight loss for diet A\n",
    "diet_B = np.random.normal(loc=3, scale=1, size=50)  # Mean weight loss for diet B\n",
    "diet_C = np.random.normal(loc=2.5, scale=1, size=50)  # Mean weight loss for diet C\n",
    "\n",
    "# Combine data into a single array\n",
    "all_weights = np.concatenate([diet_A, diet_B, diet_C])\n",
    "\n",
    "# Create a corresponding group indicator array\n",
    "groups = ['A'] * 50 + ['B'] * 50 + ['C'] * 50\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Report the results\n",
    "print(\"One-Way ANOVA Results:\")\n",
    "print(\"F-Statistic:\", f_statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\"\\nThere is a significant difference in mean weight loss between at least two diets.\")\n",
    "else:\n",
    "    print(\"\\nNo significant difference in mean weight loss between the diets was found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a438b9-8b14-4663-86e5-4dffdd3807f9",
   "metadata": {},
   "source": [
    "# Answer10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0611e927-be36-40b1-a122-09a0f99a7399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-Way ANOVA Results:\n",
      "                         sum_sq    df         F    PR(>F)\n",
      "Program                8.337633   2.0  0.193670  0.824297\n",
      "Experience            31.851905   1.0  1.479736  0.227223\n",
      "Program:Experience    52.479686   2.0  1.219018  0.300694\n",
      "Residual            1808.132913  84.0       NaN       NaN\n",
      "\n",
      "Interpretation:\n",
      "No significant main effect of the software program on the task completion time was found.\n",
      "No significant main effect of employee experience level on the task completion time was found.\n",
      "No significant interaction effect between software program and employee experience was found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Example data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Generate synthetic data\n",
    "data = {\n",
    "    'Program': np.random.choice(['A', 'B', 'C'], size=90),\n",
    "    'Experience': np.random.choice(['Novice', 'Experienced'], size=90),\n",
    "    'Time': np.random.normal(loc=20, scale=5, size=90)  \n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "model = ols('Time ~ Program * Experience', data=df).fit()\n",
    "\n",
    "# Perform the ANOVA\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Report the results\n",
    "print(\"Two-Way ANOVA Results:\")\n",
    "print(anova_table)\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nInterpretation:\")\n",
    "if anova_table['PR(>F)']['Program'] < 0.05:\n",
    "    print(\"There is a significant main effect of the software program on the task completion time.\")\n",
    "else:\n",
    "    print(\"No significant main effect of the software program on the task completion time was found.\")\n",
    "\n",
    "if anova_table['PR(>F)']['Experience'] < 0.05:\n",
    "    print(\"There is a significant main effect of employee experience level on the task completion time.\")\n",
    "else:\n",
    "    print(\"No significant main effect of employee experience level on the task completion time was found.\")\n",
    "\n",
    "if anova_table['PR(>F)']['Program:Experience'] < 0.05:\n",
    "    print(\"There is a significant interaction effect between software program and employee experience.\")\n",
    "else:\n",
    "    print(\"No significant interaction effect between software program and employee experience was found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23bcda-52d0-49d7-bf6e-c606885b2c3f",
   "metadata": {},
   "source": [
    "# Answer11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb622617-df63-4059-abbf-f642a8e69a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-Sample T-Test Results:\n",
      "T-Statistic: -4.754695943505281\n",
      "P-Value: 3.819135262679478e-06\n",
      "\n",
      "There is a significant difference in test scores between the control and experimental groups.\n",
      "Follow-up post-hoc tests are not needed in this case because there are only two groups.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Example data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "control_group = np.random.normal(loc=70, scale=10, size=100)  # Control group test scores\n",
    "experimental_group = np.random.normal(loc=75, scale=10, size=100)  # Experimental group test scores\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = ttest_ind(control_group, experimental_group)\n",
    "\n",
    "# Report the results\n",
    "print(\"Two-Sample T-Test Results:\")\n",
    "print(\"T-Statistic:\", t_statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\"\\nThere is a significant difference in test scores between the control and experimental groups.\")\n",
    "    print(\"Follow-up post-hoc tests are not needed in this case because there are only two groups.\")\n",
    "else:\n",
    "    print(\"\\nNo significant difference in test scores between the groups was found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bac683-0608-4e82-bd02-b2da83822b69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
